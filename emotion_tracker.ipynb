{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from emonet.models import EmoNet\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "import torchvision.transforms as T\n",
    "\n",
    "t_p_transform = T.ToPILImage()\n",
    "transform_image = transforms.Compose([transforms.ToTensor()])\n",
    "mtcnn = MTCNN(keep_all=True)\n",
    "classes = {0:\"Neutral\", 1:\"Happy\", 2:\"Sad\", 3:\"Surprise\", 4:\"Fear\", 5:\"Disgust\", 6:\"Anger\", 7:\"Contempt\"}\n",
    "n_expression=8\n",
    "image_size = 256\n",
    "state_dict_path = Path().parent.joinpath('pretrained', f'emonet_{n_expression}.pth')\n",
    "state_dict = torch.load(str(state_dict_path), map_location='cpu')\n",
    "state_dict = {k.replace('module.',''):v for k,v in state_dict.items()}\n",
    "net = EmoNet(n_expression=n_expression).to(\"cpu\")\n",
    "net.load_state_dict(state_dict, strict=False)\n",
    "net.eval()\n",
    "\n",
    "def recognize_emotion(pil_image, mtcnn, emonet):\n",
    "    boxes, probs, points = mtcnn.detect(pil_image, landmarks=True)\n",
    "    box = boxes[0]\n",
    "    t_image = pil_image.crop(box.tolist())\n",
    "    centercrop = torchvision.transforms.CenterCrop(np.min(t_image.size))\n",
    "    resize = torchvision.transforms.Resize(256)\n",
    "    t = transform_image(t_image)\n",
    "    t = centercrop(t)\n",
    "    t = resize(t)\n",
    "    out = emonet(t[None, :])\n",
    "    val = out['valence']\n",
    "    ar = out['arousal']\n",
    "    expr = out['expression']\n",
    "    val = np.squeeze(val.detach().numpy()).item()\n",
    "    ar = np.squeeze(ar.detach().numpy()).item()\n",
    "    expr = np.argmax(np.squeeze(expr.detach().numpy()))\n",
    "    return val, ar, classes[expr], t_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "Valence: 0.20047259628772734\n",
      "Arousal: 0.01847994327545166\n",
      "['Neutral', 'Neutral', 'Neutral', 'Neutral', 'Happy']\n"
     ]
    }
   ],
   "source": [
    "min = 17\n",
    "sec = 41\n",
    "duration = 5\n",
    "ltime = min*60+sec\n",
    "cap = cv2.VideoCapture('s2.mp4')\n",
    "print(cap.get(cv2.CAP_PROP_FPS))\n",
    "vals = []\n",
    "arls = []\n",
    "emotions = []\n",
    "for i in range(duration):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 25 * (ltime + i))\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = Image.fromarray(frame)\n",
    "    #display(frame)\n",
    "    val, arousal, emotion, cropped_image = recognize_emotion(frame, mtcnn, net)\n",
    "    vals.append(val)\n",
    "    arls.append(arousal)\n",
    "    emotions.append(emotion)\n",
    "\n",
    "print(f\"Valence: {statistics.mean(vals)}\")\n",
    "print(f\"Arousal: {statistics.mean(arls)}\")\n",
    "print(emotions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}