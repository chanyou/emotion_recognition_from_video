{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import csv\n",
    "import statistics\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from emonet.models import EmoNet\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "import torchvision.transforms as T\n",
    "from statistics import mode\n",
    "\n",
    "t_p_transform = T.ToPILImage()\n",
    "transform_image = transforms.Compose([transforms.ToTensor()])\n",
    "mtcnn = MTCNN(keep_all=True)\n",
    "classes = {0:\"Neutral\", 1:\"Happy\", 2:\"Sad\", 3:\"Surprise\", 4:\"Fear\", 5:\"Disgust\", 6:\"Anger\", 7:\"Contempt\"}\n",
    "n_expression=8\n",
    "image_size = 256\n",
    "state_dict_path = Path().parent.joinpath('pretrained', f'emonet_{n_expression}.pth')\n",
    "state_dict = torch.load(str(state_dict_path), map_location='cpu')\n",
    "state_dict = {k.replace('module.',''):v for k,v in state_dict.items()}\n",
    "net = EmoNet(n_expression=n_expression).to(\"cpu\")\n",
    "net.load_state_dict(state_dict, strict=False)\n",
    "net.eval()\n",
    "\n",
    "def recognize_emotion(pil_image, mtcnn, emonet):\n",
    "    boxes, probs, points = mtcnn.detect(pil_image, landmarks=True)\n",
    "    box = boxes[0]\n",
    "    t_image = pil_image.crop(box.tolist())\n",
    "    centercrop = torchvision.transforms.CenterCrop(np.min(t_image.size))\n",
    "    resize = torchvision.transforms.Resize(256)\n",
    "    t = transform_image(t_image)\n",
    "    t = centercrop(t)\n",
    "    t = resize(t)\n",
    "    out = emonet(t[None, :])\n",
    "    val = out['valence']\n",
    "    ar = out['arousal']\n",
    "    expr = out['expression']\n",
    "    val = np.squeeze(val.detach().numpy()).item()\n",
    "    ar = np.squeeze(ar.detach().numpy()).item()\n",
    "    expr = np.argmax(np.squeeze(expr.detach().numpy()))\n",
    "    return val, ar, classes[expr], t_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1.mp4\n",
      "s2.mp4\n",
      "s3.mp4\n"
     ]
    }
   ],
   "source": [
    "def snapshot_emotion(video, frame_sec, frame_rate, window = 5):\n",
    "    vals = []\n",
    "    arls = []\n",
    "    emotions = []\n",
    "    for i in range(window):\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_rate * (frame_sec + i * 0.5))\n",
    "        ret, frame = video.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        #display(frame)\n",
    "        val, arousal, emotion, cropped_image = recognize_emotion(frame, mtcnn, net)\n",
    "        vals.append(val)\n",
    "        arls.append(arousal)\n",
    "        emotions.append(emotion)\n",
    "    return vals, arls, emotions\n",
    "\n",
    "\n",
    "\n",
    "result = []\n",
    "with open(\"target_time.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        filename = row[0]\n",
    "\n",
    "        video = cv2.VideoCapture('s2.mp4')\n",
    "        video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        print(filename)\n",
    "        for i in range(1, len(row)):\n",
    "            if ':' not in row[i]:\n",
    "                break\n",
    "            timestr = row[i]\n",
    "            timeelems = timestr.split(':')\n",
    "            hour = int(timeelems[0])\n",
    "            min = int(timeelems[1])\n",
    "            sec = int(timeelems[2])\n",
    "            frame_sec = hour * 60 * 60 + min * 60 + sec\n",
    "            vals = []\n",
    "            arls = []\n",
    "            emotions = []\n",
    "            res = snapshot_emotion(video, frame_sec, video_fps)\n",
    "            result.append({\"filename\":filename, \"time\":row[i], \"valence\":statistics.mean(res[0]), \"arousal\":statistics.mean(res[1]), \"emotion\":mode(res[2])})\n",
    "\n",
    "\n",
    "with open(\"result.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=  result[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}